import numpy as np
import pandas as pd

train_df = pd.read_csv('/input/train.csv',index_col = 0)
test_df = pd.read_csv('/input/test.csv',index_col = 0)

train_df.head()

#数据平滑化
prices = pd.DataFrame({'price':train_df['SalePrice'],'log(price+1)':np.loglp(train_df['SalePrice'])}) 
prices.hist()
#合并数据，预处理
y_train = np.log1p(train_df.pop('SalePrice'))
all_df = pd.contact((train_df,test_df),axis = 0)
all_df.shape()
all_df['MSSubClass'].dtypes
all_df['MSSubClass'] = all_df['MSSubClass'].astype(str)
all_df['MSSubClass'].value_counts()

#category变numberical
pd.get_dummies(all_df['MSSubClass'],prefix='MSSubClass').head()
all_dummy_df = pd.get_dummies(all_df)
all_dummy_df.head()
#处理缺失数据
all_dummy_df.isnull().sum().sort_values(ascending=False).head()
mean_col = all_dummy_df.mean()
all_dummy_df = all_dummy_df.fillna(mean_col)
#由于使用的分类器是regression把numerical数据标准化
numeric_cols = all_df.columns[all_df.dtypes!='object']
numeric_col_means = all_dummy_df.loc[:,numeric_cols].mean()
numeric_col_std = all_dummy_df.loc[:,numeric_cols].std()
all_dummy_df.loc[:,numeric_cols] = (all_dummy_df.loc[:,numeric_cols]-numeric_col_means)/numeric_col_std

#建立模型
dummy_train_df = all_dummy_df.loc[train_df.index]
dummy_test_df = all_dummy_df.loc[test_df.index]
dummy_train_df.shape,dummy_test_df.shape

#Ridge
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score
X_train = dummy_train_df.values
X_test = dummy_test_df.values
#调参
alphas = np.logspace(-3,2,50)
test_scores = []
for alpha in alphas:
    clf = Ridge(alpha)
    test_score = np.sqrt(-cross_val_score(clf,X_train,y_train,cv=10,scoring='neg_mean_squared_error'))
    test_scores.append(np.mean(test_score))
#RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor
#调参
max_features = [0.1,0.3,0.5,0.7,0.9,0.99]
test_scores = []
for max_feature in max_features:
    clf = RandomForestRegressor(n_estimators=200,max_feature)
    test_score = np.sqrt(-cross_val_score(clf,X_train,y_train,cv=5,scoring='neg_mean_squared_error'))
    test_scores.append(np.mean(test_score))
plt.plot(max_features,test_scores)
plt.title('Max Features vs CV Error')

ridge = Ridge(alpha=15)
rf = RandomForestRegressor(n_estimators=500,max_features=0.3)
ridge.fit(X_train,y_train)
rf.fit(X_train,y_train)

#还原结果
y_ridge = np.expml(ridge.predict(X_test))
y_rf = np.expml(rf.predict(X_test))

#Bagging
from sklearn.ensemble import BaggingRegressor
from sklearn.model_selection import cross_val_score
params = [1,10,15,20,25,30,40]
test_scores = []
for param in params:
    clf = BaggingRegressor(n_estimators=param,base_eatimator=ridge)
    test_score = np.sqrt(-cross_val_score(clf,X_train,y_train,cv=5,scoring='neg_mean_squared_error'))
    test_scores.append(np.mean(test_score))

plt.plot(params,test_scores)
plt.title('n_estimators vs CV Error')

#Boosting
from sklearn.ensemble import AdaBoostRegressor
params = [1,10,15,20,25,30,35,40,45,50]
test_scores = []
for param in params:
    clf = BaggingRegressor(n_estimators=param,base_eatimator=ridge)
    test_score = np.sqrt(-cross_val_score(clf,X_train,y_train,cv=10,scoring='neg_mean_squared_error'))
    test_scores.append(np.mean(test_score))
    
#XGBoost
from xgboost import XGBRegressor
params = [1,2,3,4,5,6]
test_scores = []
for param in params:
    clf = XGBRegressor(max_depth=param)
    test_score = np.sqrt(-cross_val_score(clf,X_train,y_train,cv=10,scoring='neg_mean_squared_error'))
    test_scores.append(np.mean(test_score))

plt.plot(params,test_scores)
submission_df = pd.DataFrame(data={'Id':test_df.index,'SalePrice':y_final})






