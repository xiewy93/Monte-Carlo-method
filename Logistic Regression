#数据在空间的分布(线性可分)
from numpy import loadtxt, where  
from pylab import scatter, show, legend, xlabel, ylabel  
  
#load the dataset  
data = loadtxt('/home/BestYun/data/data1.txt', delimiter=',')  
  
X = data[:, 0:2]  
y = data[:, 2]  
  
pos = where(y == 1)  
neg = where(y == 0)  
scatter(X[pos, 0], X[pos, 1], marker='o', c='b')  
scatter(X[neg, 0], X[neg, 1], marker='x', c='r')  
xlabel('Feature1/Exam 1 score')  
ylabel('Feature2/Exam 2 score')  
legend(['Fail', 'Pass'])  
show()
#sigmoid函数、代价函数、和梯度下降的程序
def sigmoid(X):  
    '''''Compute sigmoid function '''  
    den =1.0+ e **(-1.0* X)  
    gz =1.0/ den  
    return gz  
def compute_cost(theta,X,y):  
    '''''computes cost given predicted and actual values'''  
    m = X.shape[0]#number of training examples  
    theta = reshape(theta,(len(theta),1))  
      
    J =(1./m)*(-transpose(y).dot(log(sigmoid(X.dot(theta))))- transpose(1-y).dot(log(1-sigmoid(X.dot(theta)))))  
      
    grad = transpose((1./m)*transpose(sigmoid(X.dot(theta))- y).dot(X))  
    #optimize.fmin expects a single value, so cannot return grad  
    return J[0][0]#,grad  
def compute_grad(theta, X, y):  
    '''''compute gradient'''  
    theta.shape =(1,3)  
    grad = zeros(3)  
    h = sigmoid(X.dot(theta.T))  
    delta = h - y  
    l = grad.size  
    for i in range(l):  
        sumdelta = delta.T.dot(X[:, i])  
        grad[i]=(1.0/ m)* sumdelta *-1  
    theta.shape =(3,)  
    return  grad  
#training data做一个预测，然后比对一下准确率
def predict(theta, X):  
    '''''Predict label using learned logistic regression parameters'''  
    m, n = X.shape  
    p = zeros(shape=(m,1))  
    h = sigmoid(X.dot(theta.T))  
    for it in range(0, h.shape[0]):  
        if h[it]>0.5:  
            p[it,0]=1  
        else:  
            p[it,0]=0  
    return p  
#Compute accuracy on our training set  
p = predict(array(theta), it)  
print'Train Accuracy: %f'%((y[where(p == y)].size / float(y.size))*100.0)

#非线性可分,对给定的两个feature做一个多项式特征的映射
def map_feature(x1, x2):  
    ''''' 
    Maps the two input features to polonomial features. 
    Returns a new feature array with more features of 
    X1, X2, X1 ** 2, X2 ** 2, X1*X2, X1*X2 ** 2, etc... 
    '''  
    x1.shape =(x1.size,1)  
    x2.shape =(x2.size,1)  
    degree =6  
    mapped_fea = ones(shape=(x1[:,0].size,1))  
    m, n = mapped_fea.shape  
    for i in range(1, degree +1):  
        for j in range(i +1):  
            r =(x1 **(i - j))*(x2 ** j)  
            mapped_fea = append(<span style="font-family: Arial, Helvetica, sans-serif;">mapped_fea</span><span style="font-family: Arial, Helvetica, sans-serif;">, r, axis=1)</span>  
    return mapped_fea  
mapped_fea = map_feature(X[:,0], X[:,1])  

#梯度下降
def cost_function_reg(theta, X, y, l):  
    '''''Compute the cost and partial derivatives as grads 
    '''  
    h = sigmoid(X.dot(theta))  
    thetaR = theta[1:,0]  
    J =(1.0/ m)*((-y.T.dot(log(h)))-((1- y.T).dot(log(1.0- h)))) \  
            +(l /(2.0* m))*(thetaR.T.dot(thetaR))  
    delta = h - y  
    sum_delta = delta.T.dot(X[:,1])  
    grad1 =(1.0/ m)* sumdelta  
    XR = X[:,1:X.shape[1]]  
    sum_delta = delta.T.dot(XR)  
    grad =(1.0/ m)*(sum_delta + l * thetaR)  
    out = zeros(shape=(grad.shape[0], grad.shape[1]+1))  
    out[:,0]= grad1  
    out[:,1:]= grad  
    return J.flatten(), out.T.flatten()  
m, n = X.shape  
y.shape =(m,1)  
it = map_feature(X[:,0], X[:,1])  
#Initialize theta parameters  
initial_theta = zeros(shape=(it.shape[1],1))  
#Use regularization and set parameter lambda to 1  
l =1  
# Compute and display initial cost and gradient for regularized logistic  
# regression  
cost, grad = cost_function_reg(initial_theta, it, y, l)  
def decorated_cost(theta):  
    return cost_function_reg(theta, it, y, l)  
print fmin_bfgs(decorated_cost, initial_theta, maxfun=500)  

#数据点上画出判定边界
u = linspace(-1,1.5,50)  
v = linspace(-1,1.5,50)  
z = zeros(shape=(len(u), len(v)))  
for i in range(len(u)):  
    for j in range(len(v)):  
        z[i, j]=(map_feature(array(u[i]), array(v[j])).dot(array(theta)))  
z = z.T  
contour(u, v, z)  
title('lambda = %f'% l)  
xlabel('Microchip Test 1')  
ylabel('Microchip Test 2')  
legend(['y = 1','y = 0','Decision boundary'])  
show()  
def predict(theta, X):  
    '''''Predict whether the label 
    is 0 or 1 using learned logistic 
    regression parameters '''  
    m, n = X.shape  
    p = zeros(shape=(m,1))  
    h = sigmoid(X.dot(theta.T))  
    for it in range(0, h.shape[0]):  
        if h[it]>0.5:  
            p[it,0]=1  
        else:  
            p[it,0]=0  
    return p  
#% Compute accuracy on our training set  
p = predict(array(theta), it)  
print'Train Accuracy: %f'%((y[where(p == y)].size / float(y.size))*100.0)  

